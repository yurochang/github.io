<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[object detection 发展综述]]></title>
      <url>/2018/02/25/object-detection-%E5%8F%91%E5%B1%95%E7%BB%BC%E8%BF%B0/</url>
      <content type="html"><![CDATA[<p>参考资料：<br>包含各领域的调研报告：<a href="http://s1nh.org/post/cv-sjtu/" target="_blank" rel="noopener">http://s1nh.org/post/cv-sjtu/</a><br><a href="https://zhuanlan.zhihu.com/p/21412911" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21412911</a><br><a href="https://www.jiqizhixin.com/articles/2017-09-18-7" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2017-09-18-7</a></p>
<p>###任务分类：</p>
<ol>
<li>物体探测也称物体检测（object detection）<br>自从 faster RCNN 后，物体定位和物体识别就同时一起做了。目前主要的开源代码是 SSD，faster RCNN, Yolo。各有优劣，SSD 和 faster RCNN 是 recall 比较高，Yolo 是 precision 比较高。 综合上来看，如果一定要选一个的话，我推荐 Yolo。注意，目前的 object detection 是用 mAP 来衡量，但 mAP 差个几个点范围内很难说明实际效果的好坏。我们组写了一个 object detection 的详细结束文件（和这份文件一起交付）。<br>另外还有一篇是刚刚出来的 Mask RCNN，性能比现在的物体检测器都好。 文章在这里 <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">https://arxiv.org/abs/1703.06870</a><br>具体中文讨论在这里 <a href="https://www.zhihu.com/question/57403701" target="_blank" rel="noopener">https://www.zhihu.com/question/57403701</a>  </li>
<li>视频中的物体探测<br>学术上这个被称为多物体跟踪（mutli-object tracking），他的基本原理是跟踪和物体检测联合训练。 主要的算法可以在这里两个网站上查到：<br>这个是专门做多物体跟踪的： <a href="https://motchallenge.net/" target="_blank" rel="noopener">https://motchallenge.net/</a><br>里面有一些有文章<br>另一个网站是： <a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php" target="_blank" rel="noopener">http://www.cvlibs.net/datasets/kitti/eval_tracking.php</a><br>这个虽然是无人车的，但原理差不多，你们可以用他们的模型<br>如果代码的话推荐这篇， <a href="http://yuxng.github.io/" target="_blank" rel="noopener">http://yuxng.github.io/</a><br>这两篇文章都有代码，<br>Learning to Track: Online Multi-Object Tracking by Decision Making.<br>Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection.<br>另外 multi-object tracking 都比较慢，如果要快点的话，可以使用 yolo 每帧做处理，然后简单复现这篇文章，就行了。<br>Seq-NMS for Video Object Detection <a href="https://arxiv.org/abs/1602.08465" target="_blank" rel="noopener">https://arxiv.org/abs/1602.08465</a></li>
</ol>
<p>#一、 object detection任务<br>1、任务综述<br>给定一张图像或者视频帧，找出其中所有目标的位置，并给出每个目标的具体类别，图片中可能有一个目标或几个目标。如下图所示<br><img src="/2018/02/25/object-detection-发展综述/1.jpg" alt=""><br>对计算机来说，目标检测最困难的部分在于计算机不清楚目标出现在图像中哪个区域。图像中的目标可能出现在任何位置，目标的形态可能存在各种各样的变化。<br>2、模型框架<br>传统的目标检测中，常见的算法是DPM（Deformable Part Model）。2013年利用深度学习算法的R-CNN（Region-based Convolutional Neural Networks）的诞生使基于深度学习的目标检测进入了一个全新的时期。R-CNN在VOC 2007测试集的mAP达到48%。从此Ross Girshick一发不可收拾，连续推出了Fast R-CNN、Faster R-CNN。受Ross Girshick启发许多基于深度学习的目标检测算法被提出 </p>
<p>#二、发展历程：</p>
<p>##一、 传统目标检测方法<br><img src="/2018/02/25/object-detection-发展综述/2.jpg" alt=""><br>如上图所示，传统目标检测的方法一般分为三个阶段：首先在给定的图像上选择一些候选的区域，然后对这些区域提取特征，最后使用训练的分类器进行分类。下面我们对这三个阶段分别进行介绍。<br>1) 区域选择<br>这一步是为了对目标的位置进行定位。由于目标可能出现在图像的任何位置，而且目标的大小、长宽比例也不确定，所以最初采用滑动窗口的策略对整幅图像进行遍历，而且需要设置不同的尺度，不同的长宽比。这种穷举的策略虽然包含了目标所有可能出现的位置，但是缺点也是显而易见的：时间复杂度太高，产生冗余窗口太多，这也严重影响后续特征提取和分类的速度和性能。（实际上由于受到时间复杂度的问题，滑动窗口的长宽比一般都是固定的设置几个，所以对于长宽比浮动较大的多类别目标检测，即便是滑动窗口遍历也不能得到很好的区域）<br>2) 特征提取<br>由于目标的形态多样性，光照变化多样性，背景多样性等因素使得设计一个鲁棒的特征并不是那么容易。然而提取特征的好坏直接影响到分类的准确性。（这个阶段常用的特征有SIFT、HOG等）<br>3) 分类器<br>主要有SVM, Adaboost等。<br>总结：传统目标检测存在的两个主要问题：一个是基于滑动窗口的区域选择策略没有针对性，时间复杂度高，窗口冗余；二是手工设计的特征对于多样性的变化并没有很好的鲁棒性。</p>
<p>##二、 使用Region Proposal方法<br>对于传统目标检测任务存在的两个主要问题，我们该如何解决呢？<br>对于滑动窗口存在的问题，region proposal提供了很好的解决方案。region proposal（候选区域）是预先找出图中目标可能出现的位置。但由于region proposal利用了图像中的纹理、边缘、颜色等信息，可以保证在选取较少窗口（几千个甚至几百个）的情况下保持较高的召回率。这大大降低了后续操作的时间复杂度，并且获取的候选窗口要比滑动窗口的质量更高（滑动窗口固定长宽比）。比较常用的region proposal算法有selective Search和edge Boxes，如果想具体了解region proposal可以看一下PAMI2015的“What makes for effective detection proposals？”<br>有了候选区域，剩下的工作实际就是对候选区域进行图像分类的工作（特征提取+分类）。对于图像分类，不得不提的是2012年ImageNet大规模视觉识别挑战赛（ILSVRC）上，机器学习泰斗Geoffrey Hinton教授带领学生Krizhevsky使用卷积神经网络将ILSVRC分类任务的Top-5 error降低到了15.3%，而使用传统方法的第二名top-5 error高达 26.2%。此后，卷积神经网络占据了图像分类任务的绝对统治地位，微软最新的ResNet和谷歌的Inception V4模型的top-5 error降到了4%以内多，这已经超越人在这个特定任务上的能力。所以目标检测得到候选区域后使用CNN对其进行图像分类是一个不错的选择。<br>2014年，RBG（Ross B. Girshick）大神使用region proposal+CNN代替传统目标检测使用的滑动窗口+手工设计特征，设计了R-CNN框架，使得目标检测取得巨大突破，并开启了基于深度学习目标检测的热潮。</p>
<p>###(1) R-CNN (CVPR2014, TPAMI2015)<br>(Region-based Convolution Networks for Accurate Object detection and Segmentation)<br>Motivation：目标检测进展缓慢，CNN在图片分类中取得重大成功<br>Contribution：应用CNN将检测问题转化成分类问题<br><img src="/2018/02/25/object-detection-发展综述/3.jpg" alt=""><br>上面的框架图清晰的给出了R-CNN的目标检测流程：<br>(1) 输入测试图像<br>(2) 利用selective search算法在图像中提取2000个左右的region proposal。<br>(3) 将每个region proposal缩放（warp）成227x227的大小并输入到CNN，将CNN的fc7层的输出作为特征。<br>(4) 将每个region proposal提取到的CNN特征输入到SVM进行分类。<br>针对上面的框架给出几点解释：</p>
<ul>
<li>上面的框架图是测试的流程图，要进行测试我们首先要训练好提取特征的CNN模型，以及用于分类的SVM：使用在ImageNet上预训练的模型（AlexNet/VGG16）进行微调得到用于特征提取的CNN模型，然后利用CNN模型对训练集提特征训练SVM。</li>
<li>对每个region proposal缩放到同一尺度是因为CNN全连接层输入需要保证维度固定。</li>
<li>上图少画了一个过程——对于SVM分好类的region proposal做边框回归（bounding-box regression)，边框回归是对region proposal进行纠正的线性回归算法，为了让region proposal提取到的窗口跟目标真实窗口更吻合。因为region proposal提取到的窗口不可能跟人手工标记那么准，如果region proposal跟目标位置偏移较大，即便是分类正确了，但是由于IoU(region proposal与Ground Truth的窗口的交集比并集的比值)低于0.5，那么相当于目标还是没有检测到。<br>小结：R-CNN在PASCAL VOC2007上的检测结果从DPM HSC的34.3%直接提升到了66%(mAP)。如此大的提升使我们看到了region proposal+CNN的巨大优势。<br>但是R-CNN框架也存在着很多问题:<br>(1) 训练分为多个阶段，步骤繁琐: 微调网络+训练SVM+训练边框回归器<br>(2) 训练耗时，占用磁盘空间大：5000张图像产生几百G的特征文件<br>(3) 速度慢: 使用GPU, VGG16模型处理一张图像需要47s。<br>针对速度慢的这个问题，SPP-NET给出了很好的解决方案。</li>
</ul>
<p>论文地址：<a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">https://arxiv.org/abs/1311.2524</a><br>代码地址：<a href="https://github.com/rbgirshick/rcnn" target="_blank" rel="noopener">https://github.com/rbgirshick/rcnn</a> </p>
<p>###2)SPP-NET (ECCV2014, TPAMI2015)<br>(Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition)<br>Motivation：CNN要求输入图片尺寸固定<br>Contribution：引入SPP层解除固定尺寸约束<br>先看一下R-CNN为什么检测速度这么慢，一张图都需要47s！仔细看下R-CNN框架发现，对图像提完region proposal（2000个左右）之后将每个proposal当成一张图像进行后续处理(CNN提特征+SVM分类)，实际上对一张图像进行了2000次提特征和分类的过程！<br>有没有方法提速呢？好像是有的，这2000个region proposal不都是图像的一部分吗，那么我们完全可以对图像提一次卷积层特征，然后只需要将region proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后将每个region proposal的卷积层特征输入到全连接层做后续操作。（对于CNN来说，大部分运算都耗在卷积操作上，这样做可以节省大量时间）。现在的问题是每个region proposal的尺度不一样，直接这样输入全连接层肯定是不行的，因为全连接层输入必须是固定的长度。SPP-NET恰好可以解决这个问题：</p>
<p>上图对应的就是SPP-NET的网络结构图，任意给一张图像输入到CNN，经过卷积操作我们可以得到卷积特征（比如VGG16最后的卷积层为conv5_3，共产生512张特征图）。图中的window是就是原图一个region proposal对应到特征图的区域，只需要将这些不同大小window的特征映射到同样的维度，将其作为全连接的输入，就能保证只对图像提取一次卷积层特征。SPP-NET使用了空间金字塔采样（spatial pyramid pooling）：将每个window划分为4<em>4, 2</em>2, 1<em>1的块，然后每个块使用max-pooling下采样，这样对于每个window经过SPP层之后都得到了一个长度为(4</em>4+2<em>2+1)</em>512维度的特征向量，将这个作为全连接层的输入进行后续操作。<br>小结：使用SPP-NET相比于R-CNN可以大大加快目标检测的速度，但是依然存在着很多问题：<br>(1) 训练分为多个阶段，步骤繁琐: 微调网络+训练SVM+训练训练边框回归器<br>(2) SPP-NET在微调网络的时候固定了卷积层，只对全连接层进行微调，而对于一个新的任务，有必要对卷积层也进行微调。（分类的模型提取的特征更注重高层语义，而目标检测任务除了语义信息还需要目标的位置信息）<br>针对这两个问题，RBG又提出Fast R-CNN, 一个精简而快速的目标检测框架。</p>
<p>###(3)、Fast R-CNN (ICCV2015)<br>Motivation：候选框的重复计算问题<br>Contribution： 加入RoI池化层、将BB回归融入网络<br>有了前边R-CNN和SPP-NET的介绍，我们直接看Fast R-CNN的框架图：<br><img src="/2018/02/25/object-detection-发展综述/4.jpg" alt=""><br>与R-CNN框架图对比，可以发现主要有两处不同：一是最后一个卷积层后加了一个ROI pooling layer，二是损失函数使用了多任务损失函数(multi-task loss)，将边框回归直接加入到CNN网络中训练。<br>(1) ROI pooling layer实际上是SPP-NET的一个精简版，SPP-NET对每个proposal使用了不同大小的金字塔映射，而ROI pooling layer只需要下采样到一个7x7的特征图。对于VGG16网络conv5_3有512个特征图，这样所有region proposal对应了一个7<em>7</em>512维度的特征向量作为全连接层的输入。<br>(2) R-CNN训练过程分为了三个阶段，而Fast R-CNN直接使用softmax替代SVM分类，同时利用多任务损失函数边框回归也加入到了网络中，这样整个的训练过程是端到端的(除去region proposal提取阶段)。<br>(3) Fast R-CNN在网络微调的过程中，将部分卷积层也进行了微调，取得了更好的检测效果。<br>小结：Fast R-CNN融合了R-CNN和SPP-NET的精髓，并且引入多任务损失函数，使整个网络的训练和测试变得十分方便。在Pascal VOC2007训练集上训练，在VOC2007测试的结果为66.9%(mAP)，如果使用VOC2007+2012训练集训练，在VOC2007上测试结果为70%（数据集的扩充能大幅提高目标检测性能）。使用VGG16每张图像总共需要3s左右。<br>缺点：region proposal的提取使用selective search，目标检测时间大多消耗在这上面（提region proposal 2~3s，而提特征分类只需0.32s），无法满足实时应用，而且并没有实现真正意义上的端到端训练测试（region proposal使用selective search先提取处来）。那么有没有可能直接使用CNN直接产生region proposal并对其分类？Faster R-CNN框架就是符合这样需要的目标检测框架。</p>
<p>论文地址：<a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">https://arxiv.org/abs/1504.08083</a><br>代码地址：<a href="https://github.com/rbgirshick/fast-rcnn" target="_blank" rel="noopener">https://github.com/rbgirshick/fast-rcnn</a> </p>
<p>###(4)、Faster R-CNN (NIPS2015)<br>(Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks)<br>Motivation： Selective Search作为一个独立的操作，速度依然不够快<br>Contribution：抛弃了Selective Search，引入了RPN网络，使得区域提名、分类、回归一起共用卷积特征，从而得到了进一步的加速。<br>在region proposal + CNN分类的这种目标检测框架中，region proposal质量好坏直接影响到目标检测任务的精度。如果找到一种方法只提取几百个或者更少的高质量的预选窗口，而且召回率很高，这不但能加快目标检测速度，还能提高目标检测的性能（假阳例少）。RPN(Region Proposal Networks)网络应运而生。<br>RPN的核心思想是使用卷积神经网络直接产生region proposal，使用的方法本质上就是滑动窗口。RPN的设计比较巧妙，RPN只需在最后的卷积层上滑动一遍，因为anchor机制和边框回归可以得到多尺度多长宽比的region proposal。<br><img src="/2018/02/25/object-detection-发展综述/5.jpg" alt=""><br>我们直接看上边的RPN网络结构图（使用了ZF模型），给定输入图像（假设分辨率为600<em>1000），经过卷积操作得到最后一层的卷积特征图（大小约为40</em>60）。在这个特征图上使用3<em>3的卷积核（滑动窗口）与特征图进行卷积，最后一层卷积层共有256个feature map，那么这个3</em>3的区域卷积后可以获得一个256维的特征向量，后边接cls layer和reg layer分别用于分类和边框回归（跟Fast R-CNN类似，只不过这里的类别只有目标和背景两个类别）。3<em>3滑窗对应的每个特征区域同时预测输入图像3种尺度（128,256,512），3种长宽比（1:1,1:2,2:1）的region proposal，这种映射的机制称为anchor。所以对于这个40</em>60的feature map，总共有约20000(40<em>60</em>9)个anchor，也就是预测20000个region proposal。<br>这样设计的好处是什么呢？虽然现在也是用的滑动窗口策略，但是：滑动窗口操作是在卷积层特征图上进行的，维度较原始图像降低了16<em>16倍（中间经过了4次2</em>2的pooling操作）；多尺度采用了9种anchor，对应了三种尺度和三种长宽比，加上后边接了边框回归，所以即便是这9种anchor外的窗口也能得到一个跟目标比较接近的region proposal。<br>NIPS2015版本的Faster R-CNN使用的检测框架是RPN网络+Fast R-CNN网络分离进行的目标检测，整体流程跟Fast R-CNN一样，只是region proposal现在是用RPN网络提取的（代替原来的selective search）。同时作者为了让RPN的网络和Fast R-CNN网络实现卷积层的权值共享，训练RPN和Fast R-CNN的时候用了4阶段的训练方法:<br>(1) 使用在ImageNet上预训练的模型初始化网络参数，微调RPN网络；<br>(2) 使用(1)中RPN网络提取region proposal训练Fast R-CNN网络；<br>(3) 使用(2)的Fast R-CNN网络重新初始化RPN, 固定卷积层进行微调；<br>(4) 固定(2)中Fast R-CNN的卷积层，使用(3)中RPN提取的region proposal微调网络。<br>权值共享后的RPN和Fast R-CNN用于目标检测精度会提高一些。<br>使用训练好的RPN网络，给定测试图像，可以直接得到边缘回归后的region proposal，根据region proposal的类别得分对RPN网络进行排序，并选取前300个窗口作为Fast R-CNN的输入进行目标检测，使用VOC07+12训练集训练，VOC2007测试集测试mAP达到73.2%（selective search + Fast R-CNN是70%）， 目标检测的速度可以达到每秒5帧（selective search+Fast R-CNN是2~3s一张）。<br>需要注意的是，最新的版本已经将RPN网络和Fast R-CNN网络结合到了一起——将RPN获取到的proposal直接连到ROI pooling层，这才是一个真正意义上的使用一个CNN网络实现端到端目标检测的框架。<br>小结：Faster R-CNN将一直以来分离的region proposal和CNN分类融合到了一起，使用端到端的网络进行目标检测，无论在速度上还是精度上都得到了不错的提高。然而Faster R-CNN还是达不到实时的目标检测，预先获取region proposal，然后在对每个proposal分类计算量还是比较大。比较幸运的是YOLO这类目标检测方法的出现让实时性也变的成为可能。<br>总的来说，从R-CNN, SPP-NET, Fast R-CNN, Faster R-CNN一路走来，基于深度学习目标检测的流程变得越来越精简，精度越来越高，速度也越来越快。可以说基于region proposal的R-CNN系列目标检测方法是当前目标最主要的一个分支。<br>论文地址：<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">https://arxiv.org/abs/1506.01497</a><br>代码地址：<a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="noopener">https://github.com/rbgirshick/py-faster-rcnn</a> </p>
<p>###(5)、R-FCN<br>还记得 Fast R-CNN 是如何通过在所有 region proposal 上共享同一个 CNN，来改善检测速度的吗？这也是设计 R-FCN 的一个动机：通过最大化共享计算来提升速度。</p>
<p>R-FCN，或称 Region-based Fully Convolutional Net（基于区域的全卷积网络），可以在每个输出之间完全共享计算。作为全卷积网络，它在模型设计过程中遇到了一个特殊的问题。</p>
<p>一方面，当对一个目标进行分类任务时，我们希望学到模型中的位置不变性（location invariance）：无论这只猫出现在图中的哪个位置，我们都想将它分类成一只猫。另一方面，当进行目标检测任务时，我们希望学习到位置可变性（location variance）：如果这只猫在左上角，那么我们希望在图像左上角这个位置画一个框。所以，问题出现了，如果想在网络中 100% 共享卷积计算的话，我们应该如何在位置不变性（location invariance）和位置可变性（location variance）之间做出权衡呢？</p>
<p>R-FCN 的解决方案：位置敏感分数图</p>
<p>每个位置敏感分数图都代表了一个目标类（object class）的一个相关位置。例如，只要是在图像右上角检测到一只猫，就会激活一个分数图（score map）。而当系统看见左下角出现一辆车时，另一个分数图也将会被激活。本质上来讲，这些分数图都是卷积特征图，它们被训练来识别每个目标的特定部位。</p>
<p>以下是 R-FCN 的工作方式：</p>
<p>1、在输入图像上运行一个 CNN（本例中使用的是 ResNet）。<br>2、添加一个全卷积层，以生成位置敏感分数图的 score bank。这里应该有 k²(C+1) 个分数图，其中，k²代表切分一个目标的相关位置的数量（比如，3²代表一个 3x3 的空间网格），C+1 代表 C 个类外加一个背景。<br>3、运行一个全卷积 region proposal 网络（RPN），以生成感兴趣区域（regions of interest，RoI）。<br>4、对于每个 RoI，我们都将其切分成同样的 k²个子区域，然后将这些子区域作为分数图。<br>5、对每个子区域，我们检查其 score bank，以判断这个子区域是否匹配具体目标的对应位置。比如，如果我们处在「上-左」子区域，那我们就会获取与这个目标「上-左」子区域对应的分数图，并且在感兴趣区域（RoI region）里对那些值取平均。对每个类我们都要进行这个过程。<br>6、一旦每个 k²子区域都具备每个类的「目标匹配」值，那么我们就可以对这些子区域求平均值，得到每个类的分数。<br>7、通过对剩下 C+1 个维度向量进行 softmax 回归，来对 RoI 进行分类。</p>
<p>下面是 R-FCN 的示意图，用 RPN 生成 RoI：<br><img src="/2018/02/25/object-detection-发展综述/6.jpg" alt=""></p>
<p>当然，即便有上述文字以及图片的解释，你可能仍然不太明白这个模型的工作方式。老实说，当你可以实际看到 R-FCN 的工作过程时，你会发现理解起来会更加简单。下面就是一个在实践中应用的 R-FCN，它正在从图中检测一个婴儿：<br><img src="/2018/02/25/object-detection-发展综述/7.jpg" alt=""></p>
<p>我们只用简单地让 R-FCN 去处理每个 region proposal，然后将其切分成子区域，在子区域上反复询问系统：「这看起来像是婴儿的『上-左』部分吗？」，「这看起来像是婴儿的『上-中』部分吗？」，「这看起来像是婴儿的『上-右』部分吗？」等等。系统会对所有类重复这个过程。如果有足够的子区域表示「是的，我的确匹配婴儿的这个部分！」那么 RoI 就会通过对所有类进行 softmax 回归的方式被分类成一个婴儿。」</p>
<p>借助这种设置，R-FCN 便能同时处理位置可变性（location variance）与位置不变性（location invariance）。它给出不同的目标区域来处理位置可变性，让每个 region proposal 都参考同一个分数图 score bank 来处理位置不变形。这些分数图应该去学习将一只猫分类成猫，而不用管这只猫在在那个位置。最好的是，由于它是全卷积的，所以这意味着网络中所有的计算都是共享的。</p>
<p>因此，R-FCN 比 Faster R-CNN 快了好几倍，并且可以达到类似的准确率。<br>论文地址:<a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="noopener">https://arxiv.org/abs/1605.06409</a><br>代码地址：<a href="https://github.com/daijifeng001/R-FCN" target="_blank" rel="noopener">https://github.com/daijifeng001/R-FCN</a> （matlab版）<br>代码地址：<a href="https://github.com/Orpine/py-R-FCN" target="_blank" rel="noopener">https://github.com/Orpine/py-R-FCN</a> （python版） </p>
<p>###(6)、Mask R-CNN<br>从结构上看，这个可以看做是在Faster RCNN上加了一个“头”：原先的只有分类和b-box回归两个“头”，现在加了个mask（用于像素分割的），这样就把上述的物体检测功能与分割融合在了一起。因为这两种工作都需要使用庞大的基础层提取特征，融合后在任务级别上有提升了速度。<br>先看一下结构：上图是整体结构，下图是添加的“头”（两种基础网络，两种头）<br><img src="/2018/02/25/object-detection-发展综述/8.jpg" alt=""><br><img src="/2018/02/25/object-detection-发展综述/9.jpg" alt=""><br>主要改进点：<br>1）基础网络的增强，ResNet和FPN<br>2）把一直使用的ROIpooling改成了ROIAlign：因为ROIpooling是一种有损失的变形，而在高层feature maps上损失一个“像素”，反映到底层图片上那就很大了。而ROIAlign使用了双线性插值（bilinear interpolation）解决这个问题，对最终效果有提升。<br>3）把mask单独作为一个头，每个类都有一个对应的mask（一个类占用最终feature map一个channel），这样就避免了分割问题中“类间竞争”</p>
<p>Mask R-CNN主要分为两个阶段：<br>（1）生成候选框区域。该流程与Faster R-CNN相同，都是使用的RPN（Region Proposal Network）。<br>（2）在候选框区域上使用RoIPool来提取特征并进行分类和边界框回归，同时为每个RoI生成了一个二元掩码。<br>这与当前大部分系统不一样，当前这些系统的类别分类依赖于 mask 的预测。我们还是沿袭了 Fast R-CNN 的精神，它将矩形框分类和坐标回归并行的进行，这么做很大的简化了R-CNN的流程。<br>注释：掩码将一个对象的空间布局进行了编码，与类标签或框架不同的是，Mast R-CNN可以通过卷积的像素对齐来使用掩码提取空间结构。</p>
<p>##三 使用回归方法<br>Faster R-CNN的方法目前是主流的目标检测方法，但是速度上并不能满足实时的要求。YOLO一类的方法慢慢显现出其重要性，这类方法使用了回归的思想，既给定输入图像，直接在图像的多个位置上回归出这个位置的目标边框以及目标类别。</p>
<p>###(1) YOLO (CVPR2016, oral)<br>(You Only Look Once: Unified, Real-Time Object Detection)<br>Motivation：先前提出的算法都是将检测问题转化为分类解决<br>Contribution：将检测回归到回归方法，提高实时性能<br><img src="/2018/02/25/object-detection-发展综述/10.jpg" alt=""></p>
<p>我们直接看上面YOLO的目标检测的流程图：<br>(1) 给个一个输入图像，首先将图像划分成7<em>7的网格<br>(2) 对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）<br>(3) 根据上一步可以预测出7</em>7*2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可。<br>可以看到整个过程非常简单，不需要中间的region proposal在找目标，直接回归便完成了位置和类别的判定。<br><img src="/2018/02/25/object-detection-发展综述/11.jpg" alt=""></p>
<p>那么如何才能做到直接在不同位置的网格上回归出目标的位置和类别信息呢？上面是YOLO的网络结构图，前边的网络结构跟GoogLeNet的模型比较类似，主要的是最后两层的结构，卷积层之后接了一个4096维的全连接层，然后后边又全连接到一个7<em>7</em>30维的张量上。实际上这7<em>7就是划分的网格数，现在要在每个网格上预测目标两个可能的位置以及这个位置的目标置信度和类别，也就是每个网格预测两个目标，每个目标的信息有4维坐标信息(中心点坐标+长宽)，1个是目标的置信度，还有类别数20(VOC上20个类别)，总共就是(4+1)</em>2+20 = 30维的向量。这样可以利用前边4096维的全图特征直接在每个网格上回归出目标检测需要的信息（边框信息加类别）。<br>小结：YOLO将目标检测任务转换成一个回归问题，大大加快了检测的速度，使得YOLO可以每秒处理45张图像。而且由于每个网络预测目标窗口时使用的是全图信息，使得false positive比例大幅降低（充分的上下文信息）。但是YOLO也存在问题：没有了region proposal机制，只使用7*7的网格回归会使得目标不能非常精准的定位，这也导致了YOLO的检测精度并不是很高。</p>
<p>###(2)SSD<br>(SSD: Single Shot MultiBox Detector)<br>Motivation：yolo S×S的网格就是一个比较启发式的策略,难以检测小目标<br>Contribution：借鉴了Faster R-CNN中的Anchor机制，使用了多尺度特征金字塔<br>上面分析了YOLO存在的问题，使用整图特征在7<em>7的粗糙网格内回归对目标的定位并不是很精准。那是不是可以结合region proposal的思想实现精准一些的定位？SSD结合YOLO的回归思想以及Faster R-CNN的anchor机制做到了这点。<br><img src="/2018/02/25/object-detection-发展综述/12.jpg" alt=""><br>上图是SSD的一个框架图，首先SSD获取目标位置和类别的方法跟YOLO一样，都是使用回归，但是YOLO预测某个位置使用的是全图的特征，SSD预测某个位置使用的是这个位置周围的特征（感觉更合理一些）。那么如何建立某个位置和其特征的对应关系呢？可能你已经想到了，使用Faster R-CNN的anchor机制。如SSD的框架图所示，假如某一层特征图(图b)大小是8</em>8，那么就使用3<em>3的滑窗提取每个位置的特征，然后这个特征回归得到目标的坐标信息和类别信息(图c)。<br>不同于Faster R-CNN，这个anchor是在多个feature map上，这样可以利用多层的特征并且自然的达到多尺度（不同层的feature map 3</em>3滑窗感受野不同）。<br>小结：SSD结合了YOLO中的回归思想和Faster R-CNN中的anchor机制，使用全图各个位置的多尺度区域特征进行回归，既保持了YOLO速度快的特性，也保证了窗口预测的跟Faster R-CNN一样比较精准。SSD在VOC2007上mAP可以达到72.1%，速度在GPU上达到58帧每秒。<br>总结：YOLO的提出给目标检测一个新的思路，SSD的性能则让我们看到了目标检测在实际应用中真正的可能性。<br>论文地址:<a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">https://arxiv.org/abs/1512.02325</a><br>代码地址：<a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">https://github.com/weiliu89/caffe/tree/ssd</a> （caffe版）<br>代码地址：<a href="https://github.com/zhreshold/mxnet-ssd" target="_blank" rel="noopener">https://github.com/zhreshold/mxnet-ssd</a> （MXnet版）<br>学习博客：<a href="http://blog.csdn.net/sinat_26917383/article/details/67639189" target="_blank" rel="noopener">http://blog.csdn.net/sinat_26917383/article/details/67639189</a> (转载)<br>学习博客：<a href="http://blog.csdn.net/Ai_Smith/article/details/52997456" target="_blank" rel="noopener">http://blog.csdn.net/Ai_Smith/article/details/52997456</a> (转载) </p>
<p>###(3)YOLO2<br>从名字来看，很明显是在YOLO基础上改进获得的。<br>最主要的改进是：<br>1）把YOLO中的直接b-box位置预测改成了Faster R-CNN中的anchor box；<br>2）把原图尺寸改成416=32<em>13，这样就有个中间cell，对应于大物体的box，<br>3）把直接坐标预测改成了预测每个cell对应的5个anchor box的offset（这样更训练更稳定了），并把分类预测与anchor box绑定<br>4）最终cell也从7</em>7增加到13*13，并且借鉴ResNet，从倒数第二阶feature map层中添加了直接映射用于后续的计算，有助于提高小物体的检测能力。</p>
<p>其中比较重要的一点是作者认为anchor box 的尺寸有更好的选择，于是就把样本库中的所有b-box的尺寸做了统计，使用k-means找出最合适的一簇框，最终选择了5个类型。另一个重要表示是offset的形式，表示了一个anchor box在一个cell中的位置计算。<br><img src="/2018/02/25/object-detection-发展综述/13.jpg" alt=""><br><img src="/2018/02/25/object-detection-发展综述/14.jpg" alt=""><br>一个经典的工作原理示意图：<br><img src="/2018/02/25/object-detection-发展综述/15.jpg" alt=""></p>
<p>作者为了强调YOLO2的实时性，直接检测《007》中的各种物体（视频）。</p>
<p>论文地址:<a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">https://arxiv.org/abs/1506.02640</a><br>代码地址：<a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">http://pjreddie.com/darknet/yolo/</a><br>学习博客：<br><a href="http://blog.csdn.net/zy1034092330/article/details/72807924" target="_blank" rel="noopener">http://blog.csdn.net/zy1034092330/article/details/72807924</a> （转载） </p>
<p>##四、提高目标检测方法<br>R-CNN系列目标检测框架和YOLO目标检测框架给了我们进行目标检测的两个基本框架。除此之外，研究人员基于这些框架从其他方面入手提出了一系列提高目标检测性能的方法。<br>(1) 难分样本挖掘（hard negative mining）<br>R-CNN在训练SVM分类器时使用了难分样本挖掘的思想，但Fast R-CNN和Faster R-CNN由于使用端到端的训练策略并没有使用难分样本挖掘（只是设置了正负样本的比例并随机抽取）。CVPR2016的Training Region-based Object Detectors with Online Hard Example Mining(oral)将难分样本挖掘(hard example mining)机制嵌入到SGD算法中，使得Fast R-CNN在训练的过程中根据region proposal的损失自动选取合适的region proposal作为正负例训练。实验结果表明使用OHEM（Online Hard Example Mining）机制可以使得Fast R-CNN算法在VOC2007和VOC2012上mAP提高 4%左右。<br>(2) 多层特征融合<br>Fast R-CNN和Faster R-CNN都是利用了最后卷积层的特征进行目标检测，而由于高层的卷积层特征已经损失了很多细节信息（pooling操作），所以在定位时不是很精准。HyperNet等一些方法则利用了CNN的多层特征融合进行目标检测，这不仅利用了高层特征的语义信息，还考虑了低层特征的细节纹理信息，使得目标检测定位更精准。<br>(3) 使用上下文信息<br>在提取region proposal特征进行目标检测时，结合region proposal上下文信息，检测效果往往会更好一些。（Object detection via a multi-region &amp; semantic segmentation-aware CNN model以及Inside-Outside Net等论文中都使用了上下文信息）</p>
<p>##五、从cvpr2016看目标检测的发展趋势</p>
<p>###（a）检测精度<br>如何提高检测精度的指标mAP？<br>代表性的工作是ResNet、ION和HyperNet</p>
<p>###（b）识别效率<br>如何提高检测速度？<br>YOLO：这个工作在识别效率方面的优势很明显，可以做到每秒钟45帧图像，处理视频是完全没有问题的</p>
<p>###（c）定位精度<br>如何产生更准确的bounding box? 如何逐步提高评价参数IOU？（Pascal VOC中，这个值为0.5）<br>LocNet：抛弃boundingbox回归，利用概率模型。<br>从单纯的一律追求检测精度，到想方法加快检测结果，到最后追求更加准确的结果。侧面反映了目标检测研究的不断进步*。</p>
<p>#六、目前结果 </p>
<p>#七、参考链接<br>一、没有<br>二、目标检测公共数据集<br>三、自己创建数据集<br>1、图像标注工具（labelImg）<br>目前的目标检测算法很多都是在PASCAL VOC数据集上训练和验证的，该标注工具可以在windows、linux上运行，标注格式与PASCAL VOC数据集的格式一样。但是有个BUG,就是000001.jpg中，没有jpg,解决方法参考这篇博客<a href="http://blog.csdn.net/ch_liu23/article/details/53558549" target="_blank" rel="noopener">http://blog.csdn.net/ch_liu23/article/details/53558549</a><br>工具地址：<a href="https://github.com/tzutalin/labelImg" target="_blank" rel="noopener">https://github.com/tzutalin/labelImg</a><br>2、图像标注工具（BBox-Label-Tool）<br>可以标注数据集，用于yolo的训练<br>工具地址：<a href="https://github.com/puzzledqs/BBox-Label-Tool" target="_blank" rel="noopener">https://github.com/puzzledqs/BBox-Label-Tool</a><br>3、图像标注工具（ImageLabel）<br>可以标注问题轮廓，制作分割数据集<br>工具地址：<a href="https://github.com/lanbing510/ImageLabel" target="_blank" rel="noopener">https://github.com/lanbing510/ImageLabel</a> </p>
<p>四、开源项目<br>1、重要资源（转载）<br>这篇博客对目标检测方面的总结很全面，推荐<br>博客地址：<a href="http://blog.csdn.net/zhang11wu4/article/details/53967688" target="_blank" rel="noopener">http://blog.csdn.net/zhang11wu4/article/details/53967688</a><br>2、目标检测算法概述（转载）<br>这篇博客对SPP介绍的比较详细，讲述了如何进行pooling<br>博客地址：<a href="http://www.cnblogs.com/venus024/p/5590044.html" target="_blank" rel="noopener">http://www.cnblogs.com/venus024/p/5590044.html</a><br>3、这篇博客对rcnn、fast rcnn、faster rcnn进行了比较，可以很清晰的看出它们的区别，并且对RPN进行了比较详细的讲解<br>博客地址：<a href="http://blog.csdn.net/ture_dream/article/details/52896452" target="_blank" rel="noopener">http://blog.csdn.net/ture_dream/article/details/52896452</a><br>4、目标检测综述（英文版）<br>这个博客对目标检测进行了综述，而且作者博客其它的文章对各个会议进行了总结，很不错。也许需要搬梯子<br>博客地址：<a href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html" target="_blank" rel="noopener">https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html</a> （目标检测）<br>博客地址：<a href="https://handong1587.github.io/deep_learning/2016/12/06/keep-up-with-new-trends.html（会议综述）" target="_blank" rel="noopener">https://handong1587.github.io/deep_learning/2016/12/06/keep-up-with-new-trends.html（会议综述）</a><br>5、自动驾驶资源<br>github地址:<a href="https://github.com/takeitallsource/awesome-autonomous-vehicles" target="_blank" rel="noopener">https://github.com/takeitallsource/awesome-autonomous-vehicles</a><br>6、Soft-NMS<br>提出新的NMS，论文里面介绍了2种，线性NMS和高斯NMS,提高了准确率<br>论文地址：<a href="https://arxiv.org/pdf/1704.04503.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.04503.pdf</a><br>代码地址：<a href="https://github.com/bharatsingh430/soft-nms" target="_blank" rel="noopener">https://github.com/bharatsingh430/soft-nms</a><br>7、手部检测器<br>该文章制作了一个实时检测人类手部的检测模型<br>代码地址：<a href="https://github.com/victordibia/handtracking" target="_blank" rel="noopener">https://github.com/victordibia/handtracking</a><br>五、移动端深度学习<br>1、tensorflow in android<br>在安卓设备上运行tensorflow，可能需要梯子<br>博客地址：<a href="https://www.oreilly.com/learning/tensorflow-on-android" target="_blank" rel="noopener">https://www.oreilly.com/learning/tensorflow-on-android</a><br>2、MobileNets<br>论文提出了一种新的未来结构形式，将原来的卷积过程分成二步进行，减少了计算量。<br>论文地址：<a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.04861.pdf</a><br>代码地址：<a href="https://github.com/rcmalli/keras-mobilenet" target="_blank" rel="noopener">https://github.com/rcmalli/keras-mobilenet</a> （非官方代码）<br>代码地址：<a href="https://github.com/pby5/MobileNet" target="_blank" rel="noopener">https://github.com/pby5/MobileNet</a> （非官方代码） </p>
<p>#八、论文代码<br>(一)使用Region Proposal方法<br>(1)、R-CNN<br>论文地址：<a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">https://arxiv.org/abs/1311.2524</a><br>代码地址：<a href="https://github.com/rbgirshick/rcnn" target="_blank" rel="noopener">https://github.com/rbgirshick/rcnn</a><br>(2)、Fast R-CNN<br>论文地址：<a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">https://arxiv.org/abs/1504.08083</a><br>代码地址：<a href="https://github.com/rbgirshick/fast-rcnn" target="_blank" rel="noopener">https://github.com/rbgirshick/fast-rcnn</a><br>(3)、Faster R-CNN<br>论文地址：<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">https://arxiv.org/abs/1506.01497</a><br>代码地址：<a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="noopener">https://github.com/rbgirshick/py-faster-rcnn</a><br>(4)、R-FCN<br>论文地址:<a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="noopener">https://arxiv.org/abs/1605.06409</a><br>代码地址：<a href="https://github.com/daijifeng001/R-FCN" target="_blank" rel="noopener">https://github.com/daijifeng001/R-FCN</a> （matlab版）<br>代码地址：<a href="https://github.com/Orpine/py-R-FCN" target="_blank" rel="noopener">https://github.com/Orpine/py-R-FCN</a> （python版） </p>
<p>（二） 使用回归方法<br>(1)、YOLO2<br>论文地址:<a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">https://arxiv.org/abs/1506.02640</a><br>代码地址：<a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">http://pjreddie.com/darknet/yolo/</a><br>学习博客：<a href="http://blog.csdn.net/zy1034092330/article/details/72807924" target="_blank" rel="noopener">http://blog.csdn.net/zy1034092330/article/details/72807924</a> （转载）<br>(2)、SSD<br>论文地址:<a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">https://arxiv.org/abs/1512.02325</a><br>代码地址：<a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">https://github.com/weiliu89/caffe/tree/ssd</a> （caffe版）<br>代码地址：<a href="https://github.com/zhreshold/mxnet-ssd" target="_blank" rel="noopener">https://github.com/zhreshold/mxnet-ssd</a> （MXnet版）<br>学习博客：<a href="http://blog.csdn.net/sinat_26917383/article/details/67639189" target="_blank" rel="noopener">http://blog.csdn.net/sinat_26917383/article/details/67639189</a> (转载)<br>学习博客：<a href="http://blog.csdn.net/Ai_Smith/article/details/52997456" target="_blank" rel="noopener">http://blog.csdn.net/Ai_Smith/article/details/52997456</a> (转载) </p>
]]></content>
      
        <categories>
            
            <category> 研究 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> CV </tag>
            
            <tag> 综述 </tag>
            
            <tag> object detection </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[故事开始了]]></title>
      <url>/2018/01/27/%E6%95%85%E4%BA%8B%E5%BC%80%E5%A7%8B%E4%BA%86/</url>
      <content type="html"><![CDATA[<p>#先尝试一下<br>刚才是大标题，现在是正文</p>
<p>666</p>
<p>其他的 以后再说吧</p>
]]></content>
      
        <categories>
            
            <category> 杂谈 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 思绪 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[ROM、PROM、EPROM、EEPROM、Flash ROM分别指什么？]]></title>
      <url>/2018/01/25/ROM%E3%80%81PROM%E3%80%81EPROM%E3%80%81EEPROM%E3%80%81Flash%20ROM%E5%88%86%E5%88%AB%E6%8C%87%E4%BB%80%E4%B9%88/</url>
      <content type="html"><![CDATA[<p>#ROM<br>  　指的是“只读存储器”，即Read-Only Memory。这是一种线路最简单半导体电路，通过掩模工艺， 一次性制造，其中的代码与数据将永久保存(除非坏掉)，不能进行修改。<br>  　这玩意一般在大批量生产时才会被用的，优点是成本低、非常低，但是其风险比较大，在产品设计时，如果调试不彻底，很容易造成几千片的费片，行内话叫“掩砸了”！ </p>
<p>#PROM<br>　　指的是“可编程只读存储器”既Programmable Red-Only Memory。这样的产品只允许写入一次，所以也被称为“一次可编程只读存储器”(One Time Progarmming ROM，OTP-ROM)。<br>　　PROM在出厂时，存储的内容全为1，用户可以根据需要将其中的某些单元写入数据0(部分的PROM在出厂时数据全为0，则用户可以将其中的部分单元写入1)， 以实现对其“编程”的目的。PROM的典型产品是“双极性熔丝结构”，如果我们想改写某些单元，则可以给这些单元通以足够大的电流，并维持一定的时间，原先的熔丝即可熔断，这样就达到了改写某些位的效果。<br>　　另外一类经典的PROM为使用“肖特基二极管”的PROM，出厂时，其中的二极管处于反向截止状态，还是用大电流的方法将反相电压加在“肖特基二极管”，造成其永久性击穿即可。 </p>
<p>#EPROM</p>
<p>　　指的是“可擦写可编程只读存储器”，即Erasable Programmable Read-Only Memory。 它的特点是具有可擦除功能，擦除后即可进行再编程，但是缺点是擦除需要使用紫外线照射一定的时间。这一类芯片特别容易识别，其封装中包含有“石英玻璃窗”，一个编程后的EPROM芯片的“石英玻璃窗”一般使用黑色不干胶纸盖住， 以防止遭到阳光直射。 </p>
<p>#EEPROM<br>　　指的是“电可擦除可编程只读存储器”，即Electrically Erasable Programmable Read-Only Memory。<br>　　它的最大优点是可直接用电信号擦除，也可用电信号写入。EEPROM不能取代RAM的原应是其工艺复杂， 耗费的门电路过多，且重编程时间比较长，同时其有效重编程次数也比较低。 </p>
<p>#Flash memory<br>　　指的是“闪存”，所谓“闪存”，它也是一种非易失性的内存，属于EEPROM的改进产品。它的最大特点是必须按块(Block)擦除(每个区块的大小不定，不同厂家的产品有不同的规格)， 而EEPROM则可以一次只擦除一个字节(Byte)。<br>　　目前“闪存”被广泛用在PC机的主板上，用来保存BIOS程序，便于进行程序的升级。其另外一大应用领域是用来作为硬盘的替代品，具有抗震、速度快、无噪声、耗电低的优点，但是将其用来取代RAM就显得不合适，因为RAM需要能够按字节改写，而Flash ROM做不到。</p>
<h1 id="然而并没有什么卵用"><a href="#然而并没有什么卵用" class="headerlink" title="然而并没有什么卵用"></a>然而并没有什么卵用</h1>]]></content>
      
        <categories>
            
            <category> 硬件 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数字电路 </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
